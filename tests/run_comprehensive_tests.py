#!/usr/bin/env python3
"""
Script de lancement des tests complets
"""

import os
import sys
import subprocess
import time
from datetime import datetime

def print_header(title):
    """Affiche un en-tÃªte formatÃ©"""
    print("\n" + "=" * 60)
    print(f"ğŸ§ª {title}")
    print("=" * 60)

def print_section(title):
    """Affiche une section formatÃ©e"""
    print(f"\nğŸ“‹ {title}")
    print("-" * 40)

def run_command(command, description):
    """ExÃ©cute une commande et affiche le rÃ©sultat"""
    print(f"ğŸ”„ {description}...")
    start_time = time.time()
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        duration = time.time() - start_time
        
        if result.returncode == 0:
            print(f"âœ… {description} - RÃ©ussi ({duration:.2f}s)")
            if result.stdout:
                print(f"ğŸ“„ Sortie: {result.stdout.strip()}")
        else:
            print(f"âŒ {description} - Ã‰chec ({duration:.2f}s)")
            if result.stderr:
                print(f"ğŸš¨ Erreur: {result.stderr.strip()}")
            return False
    except Exception as e:
        print(f"âŒ {description} - Exception: {e}")
        return False
    
    return True

def check_dependencies():
    """VÃ©rifie les dÃ©pendances"""
    print_section("VÃ©rification des dÃ©pendances")
    
    dependencies = [
        ("python", "Python 3.8+"),
        ("pip", "Pip"),
        ("pytest", "Pytest"),
        ("requests", "Requests"),
        ("sqlalchemy", "SQLAlchemy"),
        ("fastapi", "FastAPI"),
        ("uvicorn", "Uvicorn"),
    ]
    
    for dep, description in dependencies:
        try:
            result = subprocess.run(f"python -c 'import {dep}'", shell=True, capture_output=True)
            if result.returncode == 0:
                print(f"âœ… {description}")
            else:
                print(f"âŒ {description} - Manquant")
                return False
        except:
            print(f"âŒ {description} - Erreur de vÃ©rification")
            return False
    
    return True

def run_unit_tests():
    """Lance les tests unitaires"""
    print_section("Tests unitaires")
    
    test_files = [
        "tests/unit/test_auth_comprehensive.py",
        "tests/unit/test_cart_comprehensive.py", 
        "tests/unit/test_orders_comprehensive.py",
        "tests/unit/test_security_comprehensive.py",
        "tests/unit/test_performance_comprehensive.py",
        "tests/unit/test_api_endpoints.py",
        "tests/unit/test_auth_service.py",
        "tests/unit/test_products.py",
        "tests/unit/test_cart.py",
        "tests/unit/test_orders.py",
        "tests/unit/test_payments.py",
        "tests/unit/test_support.py",
    ]
    
    success_count = 0
    total_count = len(test_files)
    
    for test_file in test_files:
        if os.path.exists(test_file):
            command = f"python -m pytest {test_file} -v --tb=short"
            if run_command(command, f"Test {test_file}"):
                success_count += 1
        else:
            print(f"âš ï¸  Fichier de test non trouvÃ©: {test_file}")
    
    print(f"\nğŸ“Š Tests unitaires: {success_count}/{total_count} rÃ©ussis")
    return success_count == total_count

def run_integration_tests():
    """Lance les tests d'intÃ©gration"""
    print_section("Tests d'intÃ©gration")
    
    test_files = [
        "tests/integration/test_database_comprehensive.py",
        "tests/integration/test_database_integration.py",
    ]
    
    success_count = 0
    total_count = len(test_files)
    
    for test_file in test_files:
        if os.path.exists(test_file):
            command = f"python -m pytest {test_file} -v --tb=short"
            if run_command(command, f"Test {test_file}"):
                success_count += 1
        else:
            print(f"âš ï¸  Fichier de test non trouvÃ©: {test_file}")
    
    print(f"\nğŸ“Š Tests d'intÃ©gration: {success_count}/{total_count} rÃ©ussis")
    return success_count == total_count

def run_e2e_tests():
    """Lance les tests end-to-end"""
    print_section("Tests end-to-end")
    
    test_files = [
        "tests/e2e/test_user_journey_comprehensive.py",
        "tests/e2e/test_final.py",
        "tests/e2e/test_user_journey.py",
    ]
    
    success_count = 0
    total_count = len(test_files)
    
    for test_file in test_files:
        if os.path.exists(test_file):
            command = f"python -m pytest {test_file} -v --tb=short"
            if run_command(command, f"Test {test_file}"):
                success_count += 1
        else:
            print(f"âš ï¸  Fichier de test non trouvÃ©: {test_file}")
    
    print(f"\nğŸ“Š Tests end-to-end: {success_count}/{total_count} rÃ©ussis")
    return success_count == total_count

def run_all_tests():
    """Lance tous les tests"""
    print_section("Tous les tests")
    
    command = "python -m pytest tests/ -v --tb=short --maxfail=5"
    return run_command(command, "Tous les tests")

def run_specific_test_categories():
    """Lance des catÃ©gories spÃ©cifiques de tests"""
    print_section("Tests par catÃ©gorie")
    
    categories = [
        ("tests/unit/", "Tests unitaires", "unit"),
        ("tests/integration/", "Tests d'intÃ©gration", "integration"),
        ("tests/e2e/", "Tests end-to-end", "e2e"),
    ]
    
    results = {}
    
    for path, name, marker in categories:
        if os.path.exists(path):
            command = f"python -m pytest {path} -v --tb=short -m {marker}"
            success = run_command(command, name)
            results[name] = success
        else:
            print(f"âš ï¸  RÃ©pertoire non trouvÃ©: {path}")
            results[name] = False
    
    return results

def run_performance_tests():
    """Lance les tests de performance"""
    print_section("Tests de performance")
    
    command = "python -m pytest tests/unit/test_performance_comprehensive.py -v --tb=short"
    return run_command(command, "Tests de performance")

def run_security_tests():
    """Lance les tests de sÃ©curitÃ©"""
    print_section("Tests de sÃ©curitÃ©")
    
    command = "python -m pytest tests/unit/test_security_comprehensive.py -v --tb=short"
    return run_command(command, "Tests de sÃ©curitÃ©")

def generate_test_report():
    """GÃ©nÃ¨re un rapport de tests"""
    print_section("GÃ©nÃ©ration du rapport")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"test_report_{timestamp}.txt"
    
    command = f"python -m pytest tests/ -v --tb=short --html=test_report_{timestamp}.html --self-contained-html"
    success = run_command(command, "GÃ©nÃ©ration du rapport HTML")
    
    if success:
        print(f"ğŸ“„ Rapport gÃ©nÃ©rÃ©: test_report_{timestamp}.html")
    
    return success

def check_code_quality():
    """VÃ©rifie la qualitÃ© du code"""
    print_section("VÃ©rification de la qualitÃ© du code")
    
    # VÃ©rification de la syntaxe Python
    python_files = []
    for root, dirs, files in os.walk("."):
        for file in files:
            if file.endswith(".py"):
                python_files.append(os.path.join(root, file))
    
    syntax_errors = 0
    for file in python_files:
        command = f"python -m py_compile {file}"
        if not run_command(command, f"Syntaxe {file}"):
            syntax_errors += 1
    
    if syntax_errors == 0:
        print("âœ… Aucune erreur de syntaxe dÃ©tectÃ©e")
    else:
        print(f"âŒ {syntax_errors} erreurs de syntaxe dÃ©tectÃ©es")
    
    return syntax_errors == 0

def main():
    """Fonction principale"""
    print_header("TESTS COMPLETS DU PROJET E-COMMERCE")
    print(f"ğŸ• DÃ©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # VÃ©rification des dÃ©pendances
    if not check_dependencies():
        print("âŒ DÃ©pendances manquantes. Veuillez les installer.")
        sys.exit(1)
    
    # VÃ©rification de la qualitÃ© du code
    if not check_code_quality():
        print("âŒ Erreurs de qualitÃ© du code dÃ©tectÃ©es.")
        sys.exit(1)
    
    # Lancement des tests
    results = {}
    
    # Tests unitaires
    results["unit"] = run_unit_tests()
    
    # Tests d'intÃ©gration
    results["integration"] = run_integration_tests()
    
    # Tests end-to-end
    results["e2e"] = run_e2e_tests()
    
    # Tests de performance
    results["performance"] = run_performance_tests()
    
    # Tests de sÃ©curitÃ©
    results["security"] = run_security_tests()
    
    # Tests par catÃ©gorie
    category_results = run_specific_test_categories()
    results.update(category_results)
    
    # GÃ©nÃ©ration du rapport
    results["report"] = generate_test_report()
    
    # RÃ©sumÃ© final
    print_header("RÃ‰SUMÃ‰ FINAL")
    
    total_tests = len(results)
    passed_tests = sum(1 for success in results.values() if success)
    
    print(f"ğŸ“Š Tests rÃ©ussis: {passed_tests}/{total_tests}")
    print(f"ğŸ“ˆ Taux de rÃ©ussite: {(passed_tests/total_tests)*100:.1f}%")
    
    print("\nğŸ“‹ DÃ©tail des rÃ©sultats:")
    for test_name, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {test_name}")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S AVEC SUCCÃˆS!")
        print("âœ… Le projet e-commerce est en excellent Ã©tat")
        print("ğŸš€ PrÃªt pour la production!")
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests} tests ont Ã©chouÃ©")
        print("ğŸ”§ Veuillez corriger les problÃ¨mes identifiÃ©s")
    
    print(f"\nğŸ• Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
